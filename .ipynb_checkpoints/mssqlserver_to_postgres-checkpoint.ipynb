{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this notebook I am presenting an approach to copy a full MS SQL Server database into a PostgreSQL database. There are some learnings that will come from the code, which are:\n",
    "\n",
    "- How to connect to the databases via Python 3 and how to retrieve data\n",
    "- How to preprocess the tables and what to keep in mind\n",
    "- How to load the data from one database to another\n",
    "\n",
    "To work with this notebook, make sure you have the following requirements fulfilled:\n",
    "- MS SQL Server with the [AdventureWorksDW2019 sample database](https://docs.microsoft.com/de-de/sql/samples/adventureworks-install-configure?view=sql-server-ver15&tabs=ssms) set up\n",
    "- PostgreSQL with empty database 'adventureworksdw2019' created\n",
    "- Python and Jupyter Notebooks\n",
    "\n",
    "You don't need SQL to work with this notebook. However, a basic understanding of SQL is definitely helpful. Additionally, I assume you're familiar with Python and its naming conventions.\n",
    "If you download the notebook and open it in Jupyter Notebooks, you can toggle the lines by pressing l.\n",
    "\n",
    "The operating system is not that important for this project. However, in my [Medium article] I am explaining pitfalls that arise when using Windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import relevant libraries\n",
    "- We're going to need pandas, b/c we will use the function *to_sql()* to load the data into our target database.\n",
    "- We'll need pyodbc to connect to MS SQL Server.\n",
    "- For credentials, which are stored in Environment Variables, we'll make use of the os library.\n",
    "- Regex, for some basic transformation.\n",
    "- And lastly, *create_engine* by sqlalchemy, to create our connection objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import os\n",
    "import re\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating engines\n",
    "To connect to our databases, we're creating the database engines via sqlalchemy. Credentials are taken from our Environment Variables and commonly shared database information (database-name or server-name) are stored in seperate variables for convenience.\n",
    "\n",
    "I worked on my local machine. Please make sure to replace this information when connecting to your own databases.\n",
    "\n",
    "To create the engine, we're storing the database-URI for each database in a separate variable. The URI has the following structure:\n",
    "*'database_system+connection_driver://username:password@host:port/database_name'*\n",
    "\n",
    "--> In case of MS SQL Server you will have to add the driver at the end of the string: *'?driver=SQL_Server_Driver'*\n",
    "\n",
    "I personally like to work with [f-strings](https://realpython.com/python-f-strings/#option-1-formatting), b/c they provide a very flexible way of inserting your expressions into any string. Just like this:\n",
    "\n",
    "```python\n",
    "expression = 'WORLD'\n",
    "print(f\"Hello, {expression.lower()}!\")\n",
    "\n",
    ">>> Hello, world!\n",
    "```\n",
    "\n",
    "They work with single quotation marks as well, but I recommend using the doubles, b/c you may want to use the singles for other purposes in the same string.\n",
    "\n",
    "Anyway, those f-strings are only available from Python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = 'adventureworksdw2019'\n",
    "mssqlserver_servername = 'xxx' # Name of the SQL Server here. If you're working on your local machine, it's the name of your machine.\n",
    "\n",
    "mssqlserver_uri = f\"mssql+pyodbc://{os.environ.get('mssqlserver_user')}:{os.environ.get('mssqlserver_pass')}@{mssqlserver_servername}/{database_name}?driver=SQL+Server\"\n",
    "mssqlserver_engine = create_engine(mssqlserver_uri)\n",
    "\n",
    "postgres_uri = f\"postgres+psycopg2://{os.environ.get('postgres_user')}:{os.environ.get('postgres_pass')}@localhost:5432/{database_name}\"\n",
    "postgres_engine = create_engine(postgres_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Retrieving basic table information from MS SQL Server\n",
    "To load our data into Postgres, we need information about tables and schemas from our source.\n",
    "As you can see, the first variable stores a SQL query string which we'll use to retrieve the table names, schema names and view names of the original AdventureWorksDW2019 database.\n",
    "\n",
    "We are achieving this by concatenating queries for both tables and views with the *UNION* operator **(lines 1-21)**. A simple JOIN of the schema will not suffice, b/c then we'd have to use a more complicated way to get our table names and view names in a single field. And we all like to work with easy to read queries, don't we?\n",
    "**Side-note:** This query is crafted to MS SQL Server. But you should be able to adjust it to any other source-database as well. If you do so, make sure to have a result set of 2 fields to keep the code working properly:\n",
    "\n",
    "table_name | schema_name \n",
    "--- | --- \n",
    "table1 | schema1\n",
    "table2 | schema1\n",
    "table3 | schema2\n",
    "... | ...\n",
    "\n",
    "Now we create a connection to SQL Server **(line 23)** by assigning a new variable and call the *connect()* method on it. This will create a connection object, which allows to start querying.\n",
    "\n",
    "Then we start the query defined before by calling the *execute()* method on our connection object and assign the result to a newly created variable **(line 25)**.\n",
    "\n",
    "To show the result set we have to call the *fetchall()* method **(line 26)**. This gets us a list of tuples in this form:\n",
    "```python\n",
    "[('AdventureWorksDWBuildVersion', 'dbo'),\n",
    " ('DatabaseLog', 'dbo'),\n",
    " ('DimAccount', 'dbo'),\n",
    " ...]\n",
    "```\n",
    "\n",
    "I recommend to transform this list into a dictionary **(line 27)**, which adds more flexibility when building the tables and schemas in the target database. We now have a dict of this form:\n",
    "```python\n",
    "{'AdventureWorksDWBuildVersion': 'dbo',\n",
    " 'DatabaseLog': 'dbo',\n",
    " 'DimAccount': 'dbo',\n",
    " ...}\n",
    "```\n",
    "\n",
    "Finally, we are storing the schema-names in a separate variable and eliminate duplicates simply by casting the values into a set **(line 29)**. Sure, we just have 'dbo' as our only schema, but keep in mind that this is not the case for regular relational databases. Hence, it is good practice to consider the schemas, regardless of the system.\n",
    "\n",
    "The last step in this process is closing the connection we have created **(line 31)**.\n",
    "\n",
    "We now have a dict with table name as key and schema name as value. And we have a set of schema names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mssqlserver_table_query = \"\"\"\n",
    "\n",
    "    SELECT\n",
    "          t.name AS table_name\n",
    "        , s.name AS schema_name\n",
    "    FROM sys.tables t\n",
    "    INNER JOIN sys.schemas s\n",
    "    ON t.schema_id = s.schema_id\n",
    "\n",
    "    UNION\n",
    "\n",
    "    SELECT\n",
    "          v.name AS table_name\n",
    "        , s.name AS schema_name\n",
    "    FROM sys.views v\n",
    "    INNER JOIN sys.schemas s\n",
    "    ON v.schema_id = s.schema_id\n",
    "\n",
    "    ORDER BY schema_name, table_name;\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "mssqlserver_connection = mssqlserver_engine.connect()\n",
    "\n",
    "mssqlserver_tables = mssqlserver_connection.execute(mssqlserver_table_query)\n",
    "mssqlserver_tables = mssqlserver_tables.fetchall()\n",
    "mssqlserver_tables = dict(mssqlserver_tables)\n",
    "\n",
    "mssqlserver_schemas = set(mssqlserver_tables.values())\n",
    "\n",
    "mssqlserver_connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Writing data into postgres\n",
    "Alright, here starts the magic. We start with a for-loop, in which we iterate through every table in our dict. We must use the *items()* method on our dict to iterate **(line 1)**.\n",
    "\n",
    "Next, we establish a connection to both SQL Server and Postgres **(lines 3 and 4)**.\n",
    "\n",
    "Next, let me draw your attention to **lines 6, 7 and 8**, regarding a variable called 'table_split'. What are we doing here and why are we doing it?\n",
    "\n",
    "I'll start with a short example: One table in the AW database is called *DimDate*. To query the table later in Postgres, we must put its name in double quotes (this applies to fields as well):\n",
    "```sql\n",
    "SELECT \"DateKey\"\n",
    "     , \"FullDateAlternateKey\"\n",
    "     , \"DayNumberOfWeek\"\n",
    "FROM \"dbo\".\"DimDate\"\n",
    "```\n",
    "This is a major inconvenience in my opinion, because as a Data Warehouse Engineer or Data Analyst you don't want to wrap every object into quotes. **To avoid that, the names must be lower-case in Postgres.** But then the table would be named *dimdate*, which I personally don't like that much because I want the table's attribute (the *dim* prefix) to be visually split from the actual name. The simplest way is to add an underscore. Therefore, *DimDate* becomes *dim_date*.\n",
    "\n",
    "We achieve this by using regex and split the name just before each uppercase-letter **(line 6)**. If *DimDate* is passed as table_name, the table_split variable becomes a list:\n",
    "```python\n",
    "table_split = [t for t in re.split(\"([A-Z][^A-Z]*)\", table_name) if t]\n",
    "\n",
    ">>> ['Dim', 'Date']\n",
    "```\n",
    "We join this list with an underscore **(line 7)** and get a string:\n",
    "```python\n",
    "table_split = '_'.join(table_split)\n",
    "\n",
    ">>> 'Dim_Date'\n",
    "```\n",
    "And finally, we turn the string into lowercase **(line 8)**:\n",
    "```python\n",
    "table_split = table_split.lower()\n",
    "\n",
    ">>> 'dim_date'\n",
    "```\n",
    "\n",
    "___\n",
    "\n",
    "We're now generating a new query string called *full_table* to retrieve all the data of the currently processed table. Again, an f-string makes life easier and combined with the dict, we can insert the schema and table name easily **(lines 10–16)**.\n",
    "\n",
    "Storing this query in a Pandas DataFrame is the next step **(lines 18)**. We call the *read_sql()* function and pass the above mentioned query string and the MS SQL Server connection to it. Our result set is now stored as a regular DataFrame (table *DimDate*):\n",
    "\n",
    "DateKey | FullDateAlternateKey | DayNumberOfWeek | EnglishDayNameOfWeek | SpanishDayNameOfWeek | ...\n",
    "--- | --- | --- | --- | --- | ---\n",
    "20050101 | 2005-01-01 | 7 | Saturday | Sábado | ...\n",
    "20050102 | 2005-01-02 | 1 | Sunday | Domingo | ...\n",
    "20050103 | 2005-01-03 | 2 | Monday | Lunes | ...\n",
    "20050104 | 2005-01-04 | 3 | Tuesday | Martes | ...\n",
    "20050105 | 2005-01-05 | 4 | Wednesday | Miércoles | ...\n",
    "... | ... | ... | ... | ...\n",
    "\n",
    "The columns still contain uppercase letters, so let's ensure that those are lowercase **(line 19)**.\n",
    "\n",
    "Finally, we call the *to_sql()* function on our DataFrame **(line 20)**.\n",
    "The parameters we're using are as follows:\n",
    "\n",
    "- **schema:** Recall, that the value of our current dict element is the schema name. So insert it here, but since we did not transform the field names like the table names, we must append the *lower()* method to the string.\\\n",
    "We don't need to create or remove existing schemas. *to_sql()* handles this own its own.\n",
    "- **name:** Here we're inserting the transformed table name, which is stored in the variable *table_split*.\n",
    "- **con:** *to_sql()* can take an engine or a connection object from sqlalchemy. I like consistency so I pass the connection of our Postgres database.\n",
    "- **chunksize:** Takes an integer as argument. Here you define the maximum number of rows processed at each iteration of *to_sql()**. I recommend to make use of this, to keep the consumed ressources of your machine low.\n",
    "- **index:** We did not set an index in the DataFrame, so *False*, b/c default is *True*.\n",
    "- **index_label:** See index.\n",
    "- **if_exists:** What should happen, if this table already exists? Since we did not craft an incremental load solution here, we must replace the data every time this function is called. Otherwise it will append the same data again and then we'll have lots of duplicates. This can happen if you run the code several times.\n",
    "\n",
    "After our *to_sql()* function is finished, our coding job is basically done. We just have to ensure that after each iteration in our loop, the existing connections are closed **(lines 22 and 23)**.\n",
    "And once all tables are dumped to Postgres (i.e. the for loop has finished), we must dispose the engines which we created at the very beginning **(lines 26 and 27)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table_name, schema_name in mssqlserver_tables.items():\n",
    "        \n",
    "    mssqlserver_connection = mssqlserver_engine.connect()\n",
    "    postgres_connection = postgres_engine.connect()\n",
    "    \n",
    "    table_split = [t for t in re.split(\"([A-Z][^A-Z]*)\", table_name) if t]\n",
    "    table_split = '_'.join(table_split)\n",
    "    table_split = table_split.lower()\n",
    "    \n",
    "    full_table = f\"\"\"\n",
    "\n",
    "        SELECT\n",
    "        *\n",
    "        FROM {schema_name}.{table_name};\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_sql(full_table, mssqlserver_connection)\n",
    "    df.columns = map(str.lower, df.columns)\n",
    "    df.to_sql(schema=schema_name.lower(), name=table_split, con=postgres_connection, chunksize=5000, index=False, index_label=False, if_exists='replace')\n",
    "\n",
    "    postgres_connection.close()\n",
    "    mssqlserver_connection.close()\n",
    "\n",
    "\n",
    "mssqlserver_engine.dispose()\n",
    "postgres_engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And that's it!\n",
    "You can run the code blocks one by one and see your Postgres database filled with the AdventureWorksDW2019 data. If you are a fan of visuals (just like me), see my other notebook about this topic, filled with print-statements to inform you about the status and stuff! :D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
